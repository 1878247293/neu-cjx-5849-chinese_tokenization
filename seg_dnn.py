# -*- coding: UTF-8 -*-
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
import math
import numpy as np
import time
import os
import traceback
from transform_data_dnn import TransformDataDNN
import constant

class SegDNN:
    """
    ä¼˜åŒ–ç‰ˆæœ¬çš„DNNä¸­æ–‡åˆ†è¯å™¨ - TensorFlow 2.xå…¼å®¹
    """
    def __init__(self, vocab_size, embed_size, skip_window):
        self.vocab_size = vocab_size
        self.embed_size = 100 # <--- å¢åŠ åµŒå…¥ç»´åº¦
        self.skip_window = skip_window
        
        # Hyperparameters
        self.alpha = 0.001
        self.h1 = 600 # <--- å¢åŠ éšè—å±‚ç¥ç»å…ƒ
        self.h2 = 300 # <--- å¢åŠ éšè—å±‚ç¥ç»å…ƒ
        self.tags_count = 4
        self.window_length = 2 * self.skip_window + 1
        self.concat_embed_size = self.embed_size * self.window_length
        self.dropout_rate = 0.5
        self.batch_size = 32
        
        # Data loader
        self.tran = TransformDataDNN(self.skip_window, vocab_size) # ä¼ é€’vocab_size
        self.dictionary = self.tran.dictionary
        
        # Build graph
        self._build_graph()

    def viterbi(self, emission, A, init_A, return_score=False):
        """
        ç»´ç‰¹æ¯”ç®—æ³•çš„å®ç°ï¼Œæ‰€æœ‰è¾“å…¥å’Œè¿”å›å‚æ•°å‡ä¸ºnumpyæ•°ç»„å¯¹è±¡
        :param emission: å‘å°„æ¦‚ç‡çŸ©é˜µï¼Œå¯¹åº”äºæœ¬æ¨¡å‹ä¸­çš„åˆ†æ•°çŸ©é˜µï¼Œ4*length
        :param A: è½¬ç§»æ¦‚ç‡çŸ©é˜µï¼Œ4*4
        :param init_A: åˆå§‹è½¬ç§»æ¦‚ç‡çŸ©é˜µï¼Œ4
        :param return_score: æ˜¯å¦è¿”å›æœ€ä¼˜è·¯å¾„çš„åˆ†å€¼ï¼Œé»˜è®¤ä¸ºFalse
        :return: æœ€ä¼˜è·¯å¾„ï¼Œè‹¥return_scoreä¸ºTrueï¼Œè¿”å›æœ€ä¼˜è·¯å¾„åŠå…¶å¯¹åº”åˆ†å€¼
        """

        length = emission.shape[1]
        path = np.ones([self.tags_count, length], dtype=np.int32) * -1
        corr_path = np.zeros([length], dtype=np.int32)
        path_score = np.ones([self.tags_count, length], dtype=np.float64) * (np.finfo('f').min / 2)
        path_score[:, 0] = init_A + emission[:, 0]

        for pos in range(1, length):
          for t in range(self.tags_count):
            for prev in range(self.tags_count):
              temp = path_score[prev][pos - 1] + A[prev][t] + emission[t][pos]
              if temp >= path_score[t][pos]:
                path[t][pos] = prev
                path_score[t][pos] = temp

        max_index = np.argmax(path_score[:, -1])
        corr_path[length - 1] = max_index
        for i in range(length - 1, 0, -1):
          max_index = path[max_index][i]
          corr_path[i - 1] = max_index
        if return_score:
          return corr_path, path_score[max_index, -1]
        else:
          return corr_path

    def _build_graph(self):
        """æ„å»ºä¼˜åŒ–çš„TensorFlowè®¡ç®—å›¾"""
        # Placeholders
        self.y = tf.compat.v1.placeholder(tf.int32, [None], name='y')
        self.is_training = tf.compat.v1.placeholder(tf.bool, name='is_training')
        self.x = tf.compat.v1.placeholder(tf.int32, [None, self.window_length], name='x')
        
        # Embedding layer
        with tf.device('/cpu:0'):
            self.embeddings = tf.Variable(
                tf.random.uniform([self.vocab_size, self.embed_size], -1.0, 1.0, dtype=tf.float32), 
                name='embeddings')
            embed = tf.nn.embedding_lookup(self.embeddings, self.x)
        
        concat_embed = tf.reshape(embed, [-1, self.concat_embed_size])
        
        # Hidden layer 1
        self.w1 = tf.Variable(tf.compat.v1.truncated_normal([self.concat_embed_size, self.h1], stddev=np.sqrt(2.0/self.concat_embed_size)), name='w1')
        self.b1 = tf.Variable(tf.zeros([self.h1]), name='b1')
        h1_out = tf.nn.relu(tf.matmul(concat_embed, self.w1) + self.b1)
        
        # Hidden layer 2
        self.w2 = tf.Variable(tf.compat.v1.truncated_normal([self.h1, self.h2], stddev=np.sqrt(2.0/self.h1)), name='w2')
        self.b2 = tf.Variable(tf.zeros([self.h2]), name='b2')
        h2_out = tf.nn.relu(tf.matmul(h1_out, self.w2) + self.b2)
        
        # Dropout
        h2_dropout = tf.cond(self.is_training,
                             lambda: tf.nn.dropout(h2_out, rate=self.dropout_rate),
                             lambda: h2_out)
        
        # Output layer
        self.w3 = tf.Variable(tf.compat.v1.truncated_normal([self.h2, self.tags_count], stddev=np.sqrt(2.0/self.h2)), name='w3')
        self.b3 = tf.Variable(tf.zeros([self.tags_count]), name='b3')
        
        # Loss function with class weights
        self.logits = tf.matmul(h2_dropout, self.w3) + self.b3
        self.word_scores = tf.nn.softmax(self.logits, axis=-1)
        
        class_weights = tf.constant([1.0, 1.0, 4.5, 1.0])
        ce_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits)
        sample_weights = tf.gather(class_weights, self.y)
        weighted_loss = ce_loss * sample_weights
        self.loss = tf.reduce_mean(weighted_loss)
        
        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in [self.w1, self.w2, self.w3]]) * 0.001
        self.total_loss = self.loss + l2_loss
        
        # Optimizer
        global_step = tf.Variable(0, trainable=False)
        learning_rate = tf.compat.v1.train.exponential_decay(self.alpha, global_step, 1000, 0.95, staircase=True)
        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)
        self.train_op = self.optimizer.minimize(self.total_loss, global_step=global_step)
        
        # CRF parameters
        self.A = tf.Variable(tf.compat.v1.truncated_normal([4, 4], stddev=0.1), name='A')
        self.init_A = tf.Variable(tf.compat.v1.truncated_normal([4], stddev=0.1), name='init_A')
        
        # Prediction and Accuracy
        self.predictions = tf.argmax(self.logits, axis=-1)
        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predictions, tf.cast(self.y, tf.int64)), tf.float32))
        self.params = [self.w1, self.b1, self.w2, self.b2, self.w3, self.b3]

    def train_optimized(self, train_data, validation_data, epochs=10, early_stopping_patience=3):
        x_data, y_data = train_data
        x_val, y_val = validation_data
        
        with tf.compat.v1.Session() as sess:
            sess.run(tf.compat.v1.global_variables_initializer())
            
            saver = tf.compat.v1.train.Saver(self.params + [self.embeddings, self.A, self.init_A], max_to_keep=1)
            
            print("ğŸš€ å¼€å§‹ä¼˜åŒ–è®­ç»ƒ (åŒ…å«éªŒè¯)...")
            
            best_val_loss = float('inf')
            patience_counter = 0
            
            for epoch in range(epochs):
                epoch_start = time.time()
                
                # --- è®­ç»ƒé˜¶æ®µ ---
                total_train_loss, total_train_acc, train_batch_count = 0, 0, 0
                num_batches = math.ceil(len(x_data) / self.batch_size)

                print(f"\nEpoch {epoch+1}/{epochs}")
                print("  Training...")
                for i, (batch_x, batch_y) in enumerate(self._get_batches((x_data, y_data))):
                    feed_dict = {self.x: batch_x, self.y: batch_y, self.is_training: True}
                    _, loss_val, acc_val = sess.run([self.train_op, self.total_loss, self.accuracy], feed_dict)
                    total_train_loss += loss_val
                    total_train_acc += acc_val
                    train_batch_count += 1
                    
                    progress = (i + 1) / num_batches
                    bar_len = 30
                    filled_len = int(round(bar_len * progress))
                    bar = 'â–ˆ' * filled_len + '-' * (bar_len - filled_len)
                    print(f"    [{bar}] {i+1}/{num_batches} - loss: {total_train_loss/train_batch_count:.4f}", end='\r')
                print() 

                avg_train_loss = total_train_loss / train_batch_count
                avg_train_acc = total_train_acc / train_batch_count

                # --- éªŒè¯é˜¶æ®µ ---
                print("  Validating...")
                total_val_loss, total_val_acc, val_batch_count = 0, 0, 0
                num_val_batches = math.ceil(len(x_val) / self.batch_size)
                for i, (batch_x, batch_y) in enumerate(self._get_batches((x_val, y_val), shuffle=False)):
                    feed_dict = {self.x: batch_x, self.y: batch_y, self.is_training: False}
                    loss_val, acc_val = sess.run([self.total_loss, self.accuracy], feed_dict)
                    total_val_loss += loss_val
                    total_val_acc += acc_val
                    val_batch_count += 1
                    
                    progress = (i + 1) / num_val_batches
                    bar_len = 30
                    filled_len = int(round(bar_len * progress))
                    bar = 'â–ˆ' * filled_len + '-' * (bar_len - filled_len)
                    print(f"    [{bar}] {i+1}/{num_val_batches}", end='\r')
                print()

                avg_val_loss = total_val_loss / val_batch_count
                avg_val_acc = total_val_acc / val_batch_count
                
                epoch_time = time.time() - epoch_start
                
                print("-" * 50)
                print(f'Epoch {epoch+1:2d}/{epochs} æ€»ç»“:')
                print(f'  Time: {epoch_time:.1f}s')
                print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}')
                print(f'  Valid Loss: {avg_val_loss:.4f}, Valid Acc: {avg_val_acc:.4f}')
                print("-" * 50)

                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    patience_counter = 0
                    print(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}ã€‚æ­£åœ¨ä¿å­˜æ¨¡å‹...")
                    saver.save(sess, 'model/best_model.ckpt')
                else:
                    patience_counter += 1
                    print(f"éªŒè¯æŸå¤±æœªæ”¹å–„ã€‚å®¹å¿è®¡æ•°: {patience_counter}/{early_stopping_patience}")
                    if patience_counter >= early_stopping_patience:
                        print(f'Early stopping at epoch {epoch+1}')
                        break
            
            print(f'âœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯Loss: {best_val_loss:.4f}')

    def _get_batches(self, data, shuffle=True):
        x_data, y_data = data
        num_samples = len(x_data)
        
        indices = np.arange(num_samples)
        if shuffle:
            np.random.shuffle(indices)
        
        for i in range(0, num_samples, self.batch_size):
            end_idx = min(i + self.batch_size, num_samples)
            batch_indices = indices[i:end_idx]
            yield x_data[batch_indices], y_data[batch_indices]

    def seg(self, sentence, model_path='model/best_model.ckpt', debug=False):
        try:
            if debug: print(f"--- å¼€å§‹åˆ†è¯: '{sentence}' ---")
            
            if not os.path.exists(model_path + '.index'):
                if debug: print(f"[Fallback] æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return self.seg_simple(sentence), [0] * len(sentence)

            feature_sequences = self.index2seq(self.sentence2index(sentence))
            
            # ä¿®æ­£: å¯¹Numpyæ•°ç»„çš„ç©ºå€¼åˆ¤æ–­
            if feature_sequences.size == 0:
                if debug: print("[Fallback] æ— æ³•ä¸ºè¾“å…¥ç”Ÿæˆç‰¹å¾åºåˆ—ã€‚")
                return self.seg_simple(sentence), [0] * len(sentence)
            
            if debug: print(f"å·²ç”Ÿæˆ {len(feature_sequences)} ä¸ªç‰¹å¾å‘é‡ï¼Œç»´åº¦: {np.array(feature_sequences).shape}ã€‚")

            with tf.compat.v1.Session() as sess:
                sess.run(tf.compat.v1.global_variables_initializer())
                
                try:
                    vars_to_restore = dict(self.params_as_dict(), **self.other_params_as_dict())
                    saver = tf.compat.v1.train.Saver(vars_to_restore)
                    saver.restore(sess, model_path)
                    if debug: print(f"âœ… æ¨¡å‹ä» {model_path} åŠ è½½æˆåŠŸã€‚")
                except Exception as e:
                    if debug:
                        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                        print("[Fallback] å› æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†è¯ã€‚")
                        traceback.print_exc()
                    return self.seg_simple(sentence), [0] * len(sentence)

                features = np.array(feature_sequences, dtype=np.int32)
                feed_dict = {self.x: features, self.is_training: False}
                
                word_scores_val, A_val, init_A_val = sess.run([self.word_scores, self.A, self.init_A], feed_dict)
                if debug: print(f"æ¨¡å‹é¢„æµ‹å®Œæˆï¼Œåˆ†æ•°çŸ©é˜µå½¢çŠ¶: {word_scores_val.shape}")
                
                tags = self.viterbi(word_scores_val.T, A_val, init_A_val)
                if debug: print(f"Viterbiè§£ç å®Œæˆï¼Œæ ‡ç­¾åºåˆ—: {tags}")

                words = self.tags2words(sentence, tags)
                if debug: print(f"åˆ†è¯ç»“æœ: {' | '.join(words)}")
                
                return words, tags
                
        except Exception as e:
            if debug:
                print(f"--- åˆ†è¯è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯ ---")
                print(f"é”™è¯¯: {e}")
                traceback.print_exc()
                print("[Fallback] å› æœªçŸ¥é”™è¯¯ï¼Œä½¿ç”¨ç®€åŒ–åˆ†è¯ã€‚")
            return self.seg_simple(sentence), [0] * len(sentence)

    # ==================================================================
    # ä»¥ä¸‹æ˜¯åŸæœ¬åœ¨SegBaseä¸­ï¼Œä½†é€»è¾‘ä¸åŒ¹é…çš„è¾…åŠ©å‡½æ•°ï¼Œç°åœ¨ç§»å…¥SegDNNå¹¶ä¿®æ­£
    # ==================================================================

    def sentence2index(self, sentence):
        """å°†å¥å­è½¬æ¢ä¸ºè¯æ±‡è¡¨ç´¢å¼•åºåˆ—"""
        indices = []
        for char in sentence:
            indices.append(self.dictionary.get(char, 0)) # ä½¿ç”¨ .get() æ›´å®‰å…¨
        return indices

    def index2seq(self, indices):
        """å°†ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºç‰¹å¾åºåˆ—ï¼ˆä½¿ç”¨æ­£ç¡®çš„å¯¹ç§°çª—å£ï¼‰"""
        if not indices:
            return []
        # ä½¿ç”¨ self.skip_window æ¥ç¡®ä¿å¯¹ç§°
        padded = [1] * self.skip_window + indices + [2] * self.skip_window
        sequences = []
        # å¾ªç¯çš„è¾¹ç•Œæ¡ä»¶ä¹Ÿè¦æ­£ç¡®
        for i in range(len(indices)):
            # ä»å¡«å……åçš„åºåˆ—ä¸­æå–çª—å£
            start = i
            end = i + 2 * self.skip_window + 1
            sequences.append(padded[start:end])
        return np.array(sequences)

    def tags2words(self, sentence, tags):
        """å°†æ ‡ç­¾åºåˆ—è½¬æ¢ä¸ºåˆ†è¯ç»“æœ"""
        words = []
        current_word = ''
        if not sentence:
            return []
        for char, tag in zip(sentence, tags):
            if tag == 0:  # S
                if current_word: words.append(current_word)
                words.append(char)
                current_word = ''
            elif tag == 1:  # B
                if current_word: words.append(current_word)
                current_word = char
            elif tag == 2:  # I
                current_word += char
            elif tag == 3:  # E
                current_word += char
                words.append(current_word)
                current_word = ''
        if current_word:
            words.append(current_word)
        return words

    def seg_simple(self, sentence):
        """ä¿®å¤åçš„ç®€åŒ–åˆ†è¯æ–¹æ³•ï¼Œä½œä¸ºæœ€åçš„å®‰å…¨å›é€€"""
        return list(sentence)

    def params_as_dict(self):
        """è·å–æ ¸å¿ƒç½‘ç»œå‚æ•°çš„å­—å…¸"""
        return {p.name.split(':')[0]: p for p in self.params}

    def other_params_as_dict(self):
        """è·å–å…¶ä»–å‚æ•°ï¼ˆå¦‚embeddings, CRFçŸ©é˜µï¼‰çš„å­—å…¸"""
        return {'embeddings': self.embeddings, 'A': self.A, 'init_A': self.init_A} 