# -*- coding: UTF-8 -*-
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
import math
import numpy as np
import time
import os
import traceback
from transform_data_dnn import TransformDataDNN
import constant

class SegDNN:
    """
    ‰ºòÂåñÁâàÊú¨ÁöÑDNN‰∏≠ÊñáÂàÜËØçÂô® - TensorFlow 2.xÂÖºÂÆπ
    """
    def __init__(self, vocab_size, embed_size, skip_window):
        self.vocab_size = vocab_size
        self.embed_size = 100 # <--- Â¢ûÂä†ÂµåÂÖ•Áª¥Â∫¶
        self.skip_window = skip_window
        
        # Hyperparameters
        self.alpha = 0.001
        self.h1 = 600 # <--- Â¢ûÂä†ÈöêËóèÂ±ÇÁ•ûÁªèÂÖÉ
        self.h2 = 300 # <--- Â¢ûÂä†ÈöêËóèÂ±ÇÁ•ûÁªèÂÖÉ
        self.tags_count = 4
        self.window_length = 2 * self.skip_window + 1
        self.concat_embed_size = self.embed_size * self.window_length
        self.dropout_rate = 0.5
        self.batch_size = 32
        
        # Data loader
        self.tran = TransformDataDNN(self.skip_window, vocab_size) # ‰º†ÈÄívocab_size
        self.dictionary = self.tran.dictionary
        
        # Build graph
        self._build_graph()

    def viterbi(self, emission, A, init_A, return_score=False):
        """
        Áª¥ÁâπÊØîÁÆóÊ≥ïÁöÑÂÆûÁé∞ÔºåÊâÄÊúâËæìÂÖ•ÂíåËøîÂõûÂèÇÊï∞Âùá‰∏∫numpyÊï∞ÁªÑÂØπË±°
        :param emission: ÂèëÂ∞ÑÊ¶ÇÁéáÁü©ÈòµÔºåÂØπÂ∫î‰∫éÊú¨Ê®°Âûã‰∏≠ÁöÑÂàÜÊï∞Áü©ÈòµÔºå4*length
        :param A: ËΩ¨ÁßªÊ¶ÇÁéáÁü©ÈòµÔºå4*4
        :param init_A: ÂàùÂßãËΩ¨ÁßªÊ¶ÇÁéáÁü©ÈòµÔºå4
        :param return_score: ÊòØÂê¶ËøîÂõûÊúÄ‰ºòË∑ØÂæÑÁöÑÂàÜÂÄºÔºåÈªòËÆ§‰∏∫False
        :return: ÊúÄ‰ºòË∑ØÂæÑÔºåËã•return_score‰∏∫TrueÔºåËøîÂõûÊúÄ‰ºòË∑ØÂæÑÂèäÂÖ∂ÂØπÂ∫îÂàÜÂÄº
        """

        length = emission.shape[1]
        path = np.ones([self.tags_count, length], dtype=np.int32) * -1
        corr_path = np.zeros([length], dtype=np.int32)
        path_score = np.ones([self.tags_count, length], dtype=np.float64) * (np.finfo('f').min / 2)
        path_score[:, 0] = init_A + emission[:, 0]

        for pos in range(1, length):
          for t in range(self.tags_count):
            for prev in range(self.tags_count):
              temp = path_score[prev][pos - 1] + A[prev][t] + emission[t][pos]
              if temp >= path_score[t][pos]:
                path[t][pos] = prev
                path_score[t][pos] = temp

        max_index = np.argmax(path_score[:, -1])
        corr_path[length - 1] = max_index
        for i in range(length - 1, 0, -1):
          max_index = path[max_index][i]
          corr_path[i - 1] = max_index
        if return_score:
          return corr_path, path_score[max_index, -1]
        else:
          return corr_path

    def _build_graph(self):
        """ÊûÑÂª∫‰ºòÂåñÁöÑTensorFlowËÆ°ÁÆóÂõæ"""
        # Placeholders
        self.y = tf.compat.v1.placeholder(tf.int32, [None], name='y')
        self.is_training = tf.compat.v1.placeholder(tf.bool, name='is_training')
        self.x = tf.compat.v1.placeholder(tf.int32, [None, self.window_length], name='x')
        
        # Embedding layer
        with tf.device('/cpu:0'):
            self.embeddings = tf.Variable(
                tf.random.uniform([self.vocab_size, self.embed_size], -1.0, 1.0, dtype=tf.float32), 
                name='embeddings')
            embed = tf.nn.embedding_lookup(self.embeddings, self.x)
        
        concat_embed = tf.reshape(embed, [-1, self.concat_embed_size])
        
        # Hidden layer 1
        self.w1 = tf.Variable(tf.compat.v1.truncated_normal([self.concat_embed_size, self.h1], stddev=np.sqrt(2.0/self.concat_embed_size)), name='w1')
        self.b1 = tf.Variable(tf.zeros([self.h1]), name='b1')
        h1_out = tf.nn.relu(tf.matmul(concat_embed, self.w1) + self.b1)
        
        # Hidden layer 2
        self.w2 = tf.Variable(tf.compat.v1.truncated_normal([self.h1, self.h2], stddev=np.sqrt(2.0/self.h1)), name='w2')
        self.b2 = tf.Variable(tf.zeros([self.h2]), name='b2')
        h2_out = tf.nn.relu(tf.matmul(h1_out, self.w2) + self.b2)
        
        # Dropout
        h2_dropout = tf.cond(self.is_training,
                             lambda: tf.nn.dropout(h2_out, rate=self.dropout_rate),
                             lambda: h2_out)
        
        # Output layer
        self.w3 = tf.Variable(tf.compat.v1.truncated_normal([self.h2, self.tags_count], stddev=np.sqrt(2.0/self.h2)), name='w3')
        self.b3 = tf.Variable(tf.zeros([self.tags_count]), name='b3')
        
        # Loss function with class weights
        self.logits = tf.matmul(h2_dropout, self.w3) + self.b3
        self.word_scores = tf.nn.softmax(self.logits, axis=-1)
        
        class_weights = tf.constant([1.0, 1.0, 4.5, 1.0])
        ce_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits)
        sample_weights = tf.gather(class_weights, self.y)
        weighted_loss = ce_loss * sample_weights
        self.loss = tf.reduce_mean(weighted_loss)
        
        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in [self.w1, self.w2, self.w3]]) * 0.001
        self.total_loss = self.loss + l2_loss
        
        # Optimizer
        global_step = tf.Variable(0, trainable=False)
        learning_rate = tf.compat.v1.train.exponential_decay(self.alpha, global_step, 1000, 0.95, staircase=True)
        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)
        self.train_op = self.optimizer.minimize(self.total_loss, global_step=global_step)
        
        # CRF parameters
        self.A = tf.Variable(tf.compat.v1.truncated_normal([4, 4], stddev=0.1), name='A')
        self.init_A = tf.Variable(tf.compat.v1.truncated_normal([4], stddev=0.1), name='init_A')
        
        # Prediction and Accuracy
        self.predictions = tf.argmax(self.logits, axis=-1)
        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predictions, tf.cast(self.y, tf.int64)), tf.float32))
        self.params = [self.w1, self.b1, self.w2, self.b2, self.w3, self.b3]

    def train_optimized(self, train_data, validation_data, epochs=10, early_stopping_patience=3):
        x_data, y_data = train_data
        x_val, y_val = validation_data
        
        with tf.compat.v1.Session() as sess:
            sess.run(tf.compat.v1.global_variables_initializer())
            
            saver = tf.compat.v1.train.Saver(self.params + [self.embeddings, self.A, self.init_A], max_to_keep=1)
            
            print("üöÄ ÂºÄÂßã‰ºòÂåñËÆ≠ÁªÉ (ÂåÖÂê´È™åËØÅ)...")
            
            best_val_loss = float('inf')
            patience_counter = 0
            
            for epoch in range(epochs):
                epoch_start = time.time()
                
                # --- ËÆ≠ÁªÉÈò∂ÊÆµ ---
                total_train_loss, total_train_acc, train_batch_count = 0, 0, 0
                num_batches = math.ceil(len(x_data) / self.batch_size)

                print(f"\nEpoch {epoch+1}/{epochs}")
                print("  Training...")
                for i, (batch_x, batch_y) in enumerate(self._get_batches((x_data, y_data))):
                    feed_dict = {self.x: batch_x, self.y: batch_y, self.is_training: True}
                    _, loss_val, acc_val = sess.run([self.train_op, self.total_loss, self.accuracy], feed_dict)
                    total_train_loss += loss_val
                    total_train_acc += acc_val
                    train_batch_count += 1
                    
                    progress = (i + 1) / num_batches
                    bar_len = 30
                    filled_len = int(round(bar_len * progress))
                    bar = '‚ñà' * filled_len + '-' * (bar_len - filled_len)
                    print(f"    [{bar}] {i+1}/{num_batches} - loss: {total_train_loss/train_batch_count:.4f}", end='\r')
                print() 

                avg_train_loss = total_train_loss / train_batch_count
                avg_train_acc = total_train_acc / train_batch_count

                # --- È™åËØÅÈò∂ÊÆµ ---
                print("  Validating...")
                total_val_loss, total_val_acc, val_batch_count = 0, 0, 0
                num_val_batches = math.ceil(len(x_val) / self.batch_size)
                for i, (batch_x, batch_y) in enumerate(self._get_batches((x_val, y_val), shuffle=False)):
                    feed_dict = {self.x: batch_x, self.y: batch_y, self.is_training: False}
                    loss_val, acc_val = sess.run([self.total_loss, self.accuracy], feed_dict)
                    total_val_loss += loss_val
                    total_val_acc += acc_val
                    val_batch_count += 1
                    
                    progress = (i + 1) / num_val_batches
                    bar_len = 30
                    filled_len = int(round(bar_len * progress))
                    bar = '‚ñà' * filled_len + '-' * (bar_len - filled_len)
                    print(f"    [{bar}] {i+1}/{num_val_batches}", end='\r')
                print()

                avg_val_loss = total_val_loss / val_batch_count
                avg_val_acc = total_val_acc / val_batch_count
                
                epoch_time = time.time() - epoch_start
                
                print("-" * 50)
                print(f'Epoch {epoch+1:2d}/{epochs} ÊÄªÁªì:')
                print(f'  Time: {epoch_time:.1f}s')
                print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}')
                print(f'  Valid Loss: {avg_val_loss:.4f}, Valid Acc: {avg_val_acc:.4f}')
                print("-" * 50)

                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    patience_counter = 0
                    print(f"üéâ Êñ∞ÁöÑÊúÄ‰Ω≥È™åËØÅÊçüÂ§±: {best_val_loss:.4f}„ÄÇÊ≠£Âú®‰øùÂ≠òÊ®°Âûã...")
                    saver.save(sess, 'model/best_model.ckpt')
                else:
                    patience_counter += 1
                    print(f"È™åËØÅÊçüÂ§±Êú™ÊîπÂñÑ„ÄÇÂÆπÂøçËÆ°Êï∞: {patience_counter}/{early_stopping_patience}")
                    if patience_counter >= early_stopping_patience:
                        print(f'Early stopping at epoch {epoch+1}')
                        break
            
            print(f'‚úÖ ËÆ≠ÁªÉÂÆåÊàê! ÊúÄ‰Ω≥È™åËØÅLoss: {best_val_loss:.4f}')

    def _get_batches(self, data, shuffle=True):
        x_data, y_data = data
        num_samples = len(x_data)
        
        indices = np.arange(num_samples)
        if shuffle:
            np.random.shuffle(indices)
        
        for i in range(0, num_samples, self.batch_size):
            end_idx = min(i + self.batch_size, num_samples)
            batch_indices = indices[i:end_idx]
            yield x_data[batch_indices], y_data[batch_indices]

    def seg(self, sentence, model_path='model/best_model.ckpt', debug=False):
        try:
            if debug: print(f"--- ÂºÄÂßãÂàÜËØç: '{sentence}' ---")
            
            if not os.path.exists(model_path + '.index'):
                if debug: print(f"[Fallback] Ê®°ÂûãÊñá‰ª∂‰∏çÂ≠òÂú®: {model_path}")
                return self.seg_simple(sentence), [0] * len(sentence)

            feature_sequences = self.index2seq(self.sentence2index(sentence))
            
            # ‰øÆÊ≠£: ÂØπNumpyÊï∞ÁªÑÁöÑÁ©∫ÂÄºÂà§Êñ≠
            if feature_sequences.size == 0:
                if debug: print("[Fallback] Êó†Ê≥ï‰∏∫ËæìÂÖ•ÁîüÊàêÁâπÂæÅÂ∫èÂàó„ÄÇ")
                return self.seg_simple(sentence), [0] * len(sentence)
            
            if debug: print(f"Â∑≤ÁîüÊàê {len(feature_sequences)} ‰∏™ÁâπÂæÅÂêëÈáèÔºåÁª¥Â∫¶: {np.array(feature_sequences).shape}„ÄÇ")

            with tf.compat.v1.Session() as sess:
                sess.run(tf.compat.v1.global_variables_initializer())
                
                try:
                    vars_to_restore = dict(self.params_as_dict(), **self.other_params_as_dict())
                    saver = tf.compat.v1.train.Saver(vars_to_restore)
                    saver.restore(sess, model_path)
                    if debug: print(f"‚úÖ Ê®°Âûã‰ªé {model_path} Âä†ËΩΩÊàêÂäü„ÄÇ")
                except Exception as e:
                    if debug:
                        print(f"‚ùå Ê®°ÂûãÂä†ËΩΩÂ§±Ë¥•: {e}")
                        print("[Fallback] Âõ†Ê®°ÂûãÂä†ËΩΩÂ§±Ë¥•Ôºå‰ΩøÁî®ÁÆÄÂåñÂàÜËØç„ÄÇ")
                        traceback.print_exc()
                    return self.seg_simple(sentence), [0] * len(sentence)

                features = np.array(feature_sequences, dtype=np.int32)
                feed_dict = {self.x: features, self.is_training: False}
                
                word_scores_val, A_val, init_A_val = sess.run([self.word_scores, self.A, self.init_A], feed_dict)
                if debug: print(f"Ê®°ÂûãÈ¢ÑÊµãÂÆåÊàêÔºåÂàÜÊï∞Áü©ÈòµÂΩ¢Áä∂: {word_scores_val.shape}")
                
                tags = self.viterbi(word_scores_val.T, A_val, init_A_val)
                if debug: print(f"ViterbiËß£Á†ÅÂÆåÊàêÔºåÊ†áÁ≠æÂ∫èÂàó: {tags}")

                words = self.tags2words(sentence, tags)
                if debug: print(f"ÂàÜËØçÁªìÊûú: {' | '.join(words)}")
                
                return words, tags
                
        except Exception as e:
            if debug:
                print(f"--- ÂàÜËØçËøáÁ®ã‰∏≠ÂèëÁîüÊú™Áü•ÈîôËØØ ---")
                print(f"ÈîôËØØ: {e}")
                traceback.print_exc()
                print("[Fallback] Âõ†Êú™Áü•ÈîôËØØÔºå‰ΩøÁî®ÁÆÄÂåñÂàÜËØç„ÄÇ")
            return self.seg_simple(sentence), [0] * len(sentence)

    # ==================================================================
    # ‰ª•‰∏ãÊòØÂéüÊú¨Âú®SegBase‰∏≠Ôºå‰ΩÜÈÄªËæë‰∏çÂåπÈÖçÁöÑËæÖÂä©ÂáΩÊï∞ÔºåÁé∞Âú®ÁßªÂÖ•SegDNNÂπ∂‰øÆÊ≠£
    # ==================================================================

    def sentence2index(self, sentence):
        """Â∞ÜÂè•Â≠êËΩ¨Êç¢‰∏∫ËØçÊ±áË°®Á¥¢ÂºïÂ∫èÂàó"""
        indices = []
        for char in sentence:
            indices.append(self.dictionary.get(char, 0)) # ‰ΩøÁî® .get() Êõ¥ÂÆâÂÖ®
        return indices

    def index2seq(self, indices):
        """Â∞ÜÁ¥¢ÂºïÂ∫èÂàóËΩ¨Êç¢‰∏∫ÁâπÂæÅÂ∫èÂàóÔºà‰ΩøÁî®Ê≠£Á°ÆÁöÑÂØπÁß∞Á™óÂè£Ôºâ"""
        if not indices:
            return []
        # ‰ΩøÁî® self.skip_window Êù•Á°Æ‰øùÂØπÁß∞
        padded = [1] * self.skip_window + indices + [2] * self.skip_window
        sequences = []
        # Âæ™ÁéØÁöÑËæπÁïåÊù°‰ª∂‰πüË¶ÅÊ≠£Á°Æ
        for i in range(len(indices)):
            # ‰ªéÂ°´ÂÖÖÂêéÁöÑÂ∫èÂàó‰∏≠ÊèêÂèñÁ™óÂè£
            start = i
            end = i + 2 * self.skip_window + 1
            sequences.append(padded[start:end])
        return np.array(sequences)

    def tags2words(self, sentence, tags):
        """Â∞ÜÊ†áÁ≠æÂ∫èÂàóËΩ¨Êç¢‰∏∫ÂàÜËØçÁªìÊûú"""
        words = []
        current_word = ''
        if not sentence:
            return []
        for char, tag in zip(sentence, tags):
            if tag == 0:  # S
                if current_word: words.append(current_word)
                words.append(char)
                current_word = ''
            elif tag == 1:  # B
                if current_word: words.append(current_word)
                current_word = char
            elif tag == 2:  # I
                current_word += char
            elif tag == 3:  # E
                current_word += char
                words.append(current_word)
                current_word = ''
        if current_word:
            words.append(current_word)
        return words

    def seg_simple(self, sentence):
        """‰øÆÂ§çÂêéÁöÑÁÆÄÂåñÂàÜËØçÊñπÊ≥ïÔºå‰Ωú‰∏∫ÊúÄÂêéÁöÑÂÆâÂÖ®ÂõûÈÄÄ"""
        return list(sentence)

    def params_as_dict(self):
        """Ëé∑ÂèñÊ†∏ÂøÉÁΩëÁªúÂèÇÊï∞ÁöÑÂ≠óÂÖ∏"""
        return {p.name.split(':')[0]: p for p in self.params}

    def other_params_as_dict(self):
        """Ëé∑ÂèñÂÖ∂‰ªñÂèÇÊï∞ÔºàÂ¶Çembeddings, CRFÁü©ÈòµÔºâÁöÑÂ≠óÂÖ∏"""
        return {'embeddings': self.embeddings, 'A': self.A, 'init_A': self.init_A} 