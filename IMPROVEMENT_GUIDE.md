 # DNN_CWS 分词效果改进指南

本指南提供了针对当前分词效果不佳问题的**三大核心改进措施**，以及完整的实施方案。

## 🎯 改进目标

针对分词结果中出现的问题：
- `稳中向好` → `稳 | 中 | 向 | 好` ❌
- `一带一路` → `一 | 带一 | 路` ❌  
- `习近平` → `习 | 近 | 平` ❌
- `双循环` → `双循 | 环` ❌

**目标：将分词准确率从当前的70-75%提升到85%以上。**

## 🔧 三大改进措施

### 1️⃣ 扩大训练数据量

**现状问题：**
- 当前PKU语料库仅有19,056行（7.4MB）
- 数据量偏小，导致模型泛化能力不足

**改进方案：**
- **合成新闻数据生成**：基于模板自动生成符合官方文稿风格的训练数据
- **专有名词增强**：针对政治、经济、科技领域补充专业术语
- **多源语料整合**：如果可用，整合MSR等其他语料库

**预期效果：**
- 数据量增长3-5倍
- 覆盖更多专业领域词汇

### 2️⃣ 改进词典构建

**现状问题：**
- 词典完全基于原始PKU语料生成
- 缺少现代政治经济术语和人名地名

**改进方案：**
- **专业词汇库**：建立包含政治人物、机构名称、政策术语的专业词库
- **复合词识别**：特别处理固定搭配和复合词
- **词典扩充**：从4000+词条扩展到6000+词条

**预期效果：**
- 专有名词识别准确率显著提升
- 减少未知词(UNK)数量

### 3️⃣ 调整标签策略

**现状问题：**
- 现有BIO标签策略对固定搭配处理不当
- 缺少特殊模式的识别规则

**改进方案：**
- **固定搭配数据库**：建立政治、经济、科技等领域的固定搭配库
- **特殊模式规则**：针对人名、地名、百分比、时间等特殊模式
- **标签策略优化**：确保固定搭配作为整体被正确标注

**预期效果：**
- 固定搭配分词准确率大幅提升
- 特殊实体识别更加准确

## 🚀 一键执行改进

### 快速开始

**最简单的方式 - 一键执行所有改进：**

```bash
python3 comprehensive_improvement.py
```

这个脚本会自动：
1. 备份原始文件
2. 依次执行三大改进措施
3. 重新训练模型（20轮）
4. 测试改进效果
5. 生成对比报告

### 分步执行（推荐用于调试）

如果你希望逐步执行并观察每个步骤的效果：

```bash
# 步骤1：扩大数据量
python3 expand_corpus.py

# 步骤2：改进词典
python3 improve_dictionary.py
cp corpus/enhanced_dict.utf8 corpus/dict.utf8

# 步骤3：改进标签策略
python3 improve_labeling.py

# 步骤4：重新处理数据
python3 init.py

# 步骤5：重新训练模型（建议20轮）
python3 train_models.py

# 步骤6：测试效果
python3 custom_seg.py
```

## 📊 效果评估

### 测试方法

使用 `test_article_official_style.txt` 作为标准测试文本：

```bash
python3 -c "
from custom_seg import batch_seg_from_file
batch_seg_from_file('test_article_official_style.txt')
"
```

### 预期改进效果

| 改进前 | 改进后 |
|--------|--------|
| `稳 \| 中 \| 向 \| 好` | `稳中向好` ✅ |
| `一 \| 带一 \| 路` | `一带一路` ✅ |
| `习 \| 近 \| 平` | `习近平` ✅ |
| `稳中求 \| 进` | `稳中求进` ✅ |
| `双循 \| 环` | `双循环` ✅ |

**整体准确率目标：从75% → 85%+**

## 🛠️ 高级调优

如果基础改进效果仍不理想，可以考虑：

### 超参数调优

编辑 `seg_dnn.py` 中的超参数：

```python
# 在 SegDNN.__init__ 中调整
self.embed_size = 150    # 增加嵌入维度
self.h1 = 800           # 增加隐藏层大小
self.h2 = 400           # 增加隐藏层大小
self.alpha = 0.0005     # 降低学习率
```

### 增加训练轮次

```python
# 在 train_models.py 中
train_dnn_model(epochs=30)  # 从10增加到30轮
```

### 模型架构升级

考虑替换为更先进的模型：
- BiLSTM-CRF
- BERT-based模型
- Transformer架构

## 📁 文件说明

本改进方案新增的文件：

```
├── expand_corpus.py           # 数据扩展工具
├── improve_dictionary.py     # 词典改进工具  
├── improve_labeling.py       # 标签策略改进工具
├── comprehensive_improvement.py # 一键改进脚本
├── IMPROVEMENT_GUIDE.md      # 本指南文档
├── corpus/
│   ├── expanded_training.utf8    # 扩展后的训练数据
│   ├── enhanced_dict.utf8        # 增强词典
│   ├── fixed_phrases.json       # 固定搭配数据库
│   └── compound_words.json      # 复合词信息
└── backup_YYYYMMDD_HHMMSS/   # 原始文件备份
```

## ⚠️ 注意事项

1. **备份重要**：改进过程会修改核心文件，请确保备份
2. **时间成本**：重新训练可能需要较长时间
3. **内存需求**：扩展数据后对内存需求会增加
4. **渐进改进**：建议分步执行，观察每步的效果

## 🎉 预期结果

完成所有改进后，您应该能看到：

✅ **专有名词分词准确**：`习近平`、`一带一路`、`中共中央政治局`  
✅ **固定搭配完整**：`稳中求进`、`高质量发展`、`新发展理念`  
✅ **数字表达正确**：`百分之五点三`、`第一季度`  
✅ **整体可读性显著提升**

**分词准确率目标：85%以上**

---

💡 **提示**：如果改进后效果仍不满意，说明可能需要考虑更换模型架构或使用更大规模的预训练模型。